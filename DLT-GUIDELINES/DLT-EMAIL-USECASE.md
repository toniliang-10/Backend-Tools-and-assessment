# Outlook Email Extraction - Complete Implementation Guide

## Table of Contents
1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Step-by-Step Implementation](#step-by-step-implementation)
4. [API Configuration](#api-configuration)
5. [Code Implementation](#code-implementation)
6. [Testing & Validation](#testing--validation)
7. [Production Deployment](#production-deployment)
8. [Troubleshooting](#troubleshooting)

---

## Overview

This guide shows you how to implement Microsoft Outlook email extraction using the DLT Service Template. The implementation extracts emails from all users in a Microsoft 365 tenant using Microsoft Graph API.

### What This Implementation Does

âœ… **Gets all mailboxes** in the tenant  
âœ… **Extracts emails** from each user's inbox  
âœ… **Handles pagination** with Microsoft Graph's `@odata.nextLink`  
âœ… **Implements checkpointing** to resume from failures  
âœ… **Supports rate limiting** with automatic retry  
âœ… **Tenant isolation** - each tenant gets separate data storage  

### Data Flow

```
1. Get OAuth token using Client Credentials
2. List all users/mailboxes in tenant
3. For each user:
   - Get emails from inbox (paginated)
   - Save checkpoint every 5 pages
   - Handle rate limits and errors
4. Store in PostgreSQL with tenant isolation
```

### Database Schema Result

After extraction, you'll have these tables in schema `outlook_emails_tenantX`:

- **`mailboxes`** - List of all users/mailboxes
- **`emails`** - All emails with metadata

---

## Prerequisites

### 1. Azure App Registration

You need to register an application in Azure AD with these permissions:

#### Required API Permissions
- **Microsoft Graph**: `User.Read.All` (Application permission)
- **Microsoft Graph**: `Mail.Read.All` (Application permission)

#### Setup Steps
1. Go to [Azure Portal](https://portal.azure.com)
2. Navigate to **Azure Active Directory** > **App registrations**
3. Click **New registration**
4. Name: `Outlook Email Extraction Service`
5. Account types: **Accounts in this organizational directory only**
6. Click **Register**

#### Configure Permissions
1. Go to **API permissions**
2. Click **Add a permission** > **Microsoft Graph** > **Application permissions**
3. Add:
   - `User.Read.All`
   - `Mail.Read.All`
4. Click **Grant admin consent**

#### Get Credentials
1. Go to **Certificates & secrets**
2. Click **New client secret**
3. Copy the **Value** (this is your `clientSecret`)
4. Copy **Application (client) ID** from **Overview** (this is your `clientId`)
5. Copy **Directory (tenant) ID** from **Overview** (this is your `tenantId`)

---

## Step-by-Step Implementation

### Step 1: Initial Setup

#### 1.1 Clone and Configure Environment
```bash
git clone <your-template-repo>
cd outlook-email-extraction
cp .env.example .env
```

#### 1.2 Edit `.env` File
```env
# Service Configuration
DLT_PIPELINE_NAME=outlook_email_extraction
DB_NAME=outlook_data
DB_SCHEMA=outlook

# Microsoft Graph API Base Settings
OUTLOOK_API_BASE_URL=https://graph.microsoft.com
OUTLOOK_API_TIMEOUT=30

# Development Settings
FLASK_ENV=development
FLASK_DEBUG=true
SECRET_KEY=dev-secret-key-change-in-production
DB_PASSWORD=password123
```

#### 1.3 Start and Test Base Setup
```bash
# Start services
docker-compose up -d

# Wait 30 seconds, then test
curl http://localhost:5000/health

# Should return healthy status
```

---

### Step 2: Update Configuration

#### 2.1 Edit `config.py`
Open `config.py` and find the HubSpot configuration section around line 50:

**Replace:**
```python
# HubSpot API Configuration
HUBSPOT_ACCESS_TOKEN=pat-na1-your-token-here
HUBSPOT_API_TIMEOUT=30
HUBSPOT_API_RATE_LIMIT=100
```

**With:**
```python
# Outlook/Microsoft Graph API settings
OUTLOOK_API_BASE_URL = 'https://graph.microsoft.com'
OUTLOOK_API_TIMEOUT = int(os.environ.get('OUTLOOK_API_TIMEOUT', 30))
OUTLOOK_API_RATE_LIMIT = int(os.environ.get('OUTLOOK_API_RATE_LIMIT', 100))
OUTLOOK_RETRY_ATTEMPTS = int(os.environ.get('OUTLOOK_RETRY_ATTEMPTS', 3))

# Note: CLIENT_ID and CLIENT_SECRET come from request auth, not environment
```

#### 2.2 Update `get_extraction_config()` Method
Find the method around line 200 and replace:

```python
@classmethod
def get_extraction_config(cls) -> Dict[str, Any]:
    return {
        # Keep existing database and DLT settings...
        'db_host': cls.DB_HOST,
        'db_port': cls.DB_PORT,
        'db_name': cls.DB_NAME,
        'db_user': cls.DB_USER,
        'db_password': cls.DB_PASSWORD,
        'db_schema': cls.DB_SCHEMA,
        
        # DLT configuration
        'pipeline_name': cls.DLT_PIPELINE_NAME,
        'working_dir': cls.DLT_WORKING_DIR,
        
        # Outlook API configuration
        'outlook_api_base_url': cls.OUTLOOK_API_BASE_URL,
        'outlook_api_timeout': cls.OUTLOOK_API_TIMEOUT,
        'outlook_retry_attempts': cls.OUTLOOK_RETRY_ATTEMPTS,
        # client_id and client_secret passed from request
    }
```

---

### Step 3: Create Outlook API Service

#### 3.1 Create API Service File
```bash
touch services/outlook_api_service.py
```

#### 3.2 Implement Outlook API Service
Copy this complete implementation into `services/outlook_api_service.py`:

```python
import requests
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
import time
import json


class OutlookAPIService:
    """
    Service for interacting with Microsoft Graph APIs for Outlook
    """
    
    def __init__(self, base_url: str = "https://graph.microsoft.com"):
        self.base_url = base_url.rstrip('/')
        self.logger = logging.getLogger(__name__)
        self.session = requests.Session()
        
        # Set default headers
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'User-Agent': 'Outlook-Email-Extraction-Service/1.0'
        })
    
    def get_access_token(self, client_id: str, client_secret: str, tenant_id: str) -> str:
        """Get OAuth token for Microsoft Graph using Client Credentials flow"""
        token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
        
        data = {
            'client_id': client_id,
            'client_secret': client_secret,
            'scope': 'https://graph.microsoft.com/.default',
            'grant_type': 'client_credentials'
        }
        
        try:
            self.logger.debug(f"Requesting access token for tenant: {tenant_id}")
            response = self.session.post(token_url, data=data)
            response.raise_for_status()
            
            token_data = response.json()
            access_token = token_data['access_token']
            
            self.logger.info(f"Successfully obtained access token for tenant: {tenant_id}")
            return access_token
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to get access token for tenant {tenant_id}: {e}")
            raise
        
    def get_mailboxes(self, access_token: str, limit: int = 100) -> Dict[str, Any]:
        """Get list of all users/mailboxes in the tenant"""
        headers = {'Authorization': f'Bearer {access_token}'}
        url = f"{self.base_url}/v1.0/users"
        
        params = {
            '$top': min(limit, 999),  # Microsoft Graph max is 999
            '$select': 'id,mail,displayName,userPrincipalName,accountEnabled',
            '$filter': 'accountEnabled eq true'  # Only get enabled accounts
        }
        
        try:
            self.logger.debug(f"Fetching mailboxes with limit: {limit}")
            response = self.session.get(url, headers=headers, params=params)
            
            # Handle rate limiting
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                self.logger.warning(f"Rate limited getting mailboxes, retrying after {retry_after} seconds")
                time.sleep(retry_after)
                response = self.session.get(url, headers=headers, params=params)
            
            response.raise_for_status()
            data = response.json()
            
            self.logger.info(f"Retrieved {len(data.get('value', []))} mailboxes")
            return data
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to get mailboxes: {e}")
            raise
    
    def get_emails(self, access_token: str, user_id: str, folder: str = 'inbox', 
                   limit: int = 100, skip_token: str = None) -> Dict[str, Any]:
        """Get emails from a specific user's mail folder"""
        headers = {'Authorization': f'Bearer {access_token}'}
        url = f"{self.base_url}/v1.0/users/{user_id}/mailFolders/{folder}/messages"
        
        params = {
            '$top': min(limit, 1000),  # Microsoft Graph max is 1000 for messages
            '$select': 'id,subject,sender,from,toRecipients,ccRecipients,receivedDateTime,sentDateTime,bodyPreview,hasAttachments,importance,isRead',
            '$orderby': 'receivedDateTime desc'
        }
        
        if skip_token:
            params['$skiptoken'] = skip_token
        
        try:
            self.logger.debug(f"Fetching emails for user {user_id}, folder: {folder}, limit: {limit}")
            response = self.session.get(url, headers=headers, params=params)
            
            # Handle rate limiting
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                self.logger.warning(f"Rate limited getting emails for user {user_id}, retrying after {retry_after} seconds")
                time.sleep(retry_after)
                response = self.session.get(url, headers=headers, params=params)
            
            # Handle user not found or access denied
            if response.status_code == 404:
                self.logger.warning(f"User {user_id} or folder '{folder}' not found")
                return {'value': []}
            elif response.status_code == 403:
                self.logger.warning(f"Access denied to user {user_id} mailbox")
                return {'value': []}
            
            response.raise_for_status()
            data = response.json()
            
            email_count = len(data.get('value', []))
            self.logger.debug(f"Retrieved {email_count} emails for user {user_id}")
            
            return data
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to get emails for user {user_id}: {e}")
            raise
    
    def validate_credentials(self, client_id: str, client_secret: str, tenant_id: str) -> bool:
        """Validate client credentials by getting a token and testing API call"""
        try:
            self.logger.info(f"Validating credentials for tenant: {tenant_id}")
            
            # Get access token
            access_token = self.get_access_token(client_id, client_secret, tenant_id)
            
            # Test API call - get first user
            mailboxes = self.get_mailboxes(access_token, limit=1)
            
            if 'value' in mailboxes:
                self.logger.info("Credentials validation successful")
                return True
            else:
                self.logger.error("Credentials validation failed - no data returned")
                return False
                
        except Exception as e:
            self.logger.error(f"Credential validation failed: {e}")
            return False
    
    def get_account_info(self, access_token: str) -> Optional[Dict[str, Any]]:
        """Get tenant/organization information"""
        try:
            headers = {'Authorization': f'Bearer {access_token}'}
            url = f"{self.base_url}/v1.0/organization"
            
            response = self.session.get(url, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                self.logger.debug("Retrieved organization info")
                return data
            else:
                self.logger.debug(f"Organization info not available: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.debug(f"Organization info not available: {str(e)}")
            return None
    
    def get_api_usage(self, access_token: str) -> Optional[Dict[str, Any]]:
        """Get API usage information from response headers"""
        try:
            # Make a simple API call to get rate limit headers
            headers = {'Authorization': f'Bearer {access_token}'}
            url = f"{self.base_url}/v1.0/me"
            
            response = self.session.get(url, headers=headers)
            
            if response.status_code == 200:
                usage_info = {}
                
                # Extract rate limit headers if available
                for header, value in response.headers.items():
                    if 'ratelimit' in header.lower() or 'throttle' in header.lower():
                        usage_info[header] = value
                
                if usage_info:
                    usage_info['timestamp'] = datetime.now(timezone.utc).isoformat()
                    return usage_info
                    
            return None
            
        except Exception as e:
            self.logger.debug(f"API usage info not available: {str(e)}")
            return None
```

---

### Step 4: Create Data Source

#### 4.1 Replace Data Source File
```bash
# Backup original
mv services/data_source.py services/data_source.py.backup
```

#### 4.2 Implement Outlook Data Source
Create new `services/data_source.py`:

```python
import dlt
import logging
from typing import Dict, List, Any, Iterator, Optional, Callable
from datetime import datetime, timezone
from urllib.parse import urlparse, parse_qs


def create_data_source(
    api_service, 
    auth_config: Dict[str, Any],
    filters: Dict[str, Any],
    checkpoint_callback: Optional[Callable] = None,
    resume_from: Optional[Dict[str, Any]] = None
):
    """
    Create DLT source function for Outlook email extraction with checkpoint support
    
    Args:
        api_service: OutlookAPIService instance
        auth_config: Authentication config from request (contains clientId, clientSecret, tenantId)
        filters: Extraction filters and configuration
        checkpoint_callback: Function to save checkpoints during extraction
        resume_from: Resume state from previous checkpoint
    """
    
    # Extract auth details from request
    client_id = auth_config.get('clientId')
    client_secret = auth_config.get('clientSecret')
    tenant_id = auth_config.get('tenantId')
    
    if not all([client_id, client_secret, tenant_id]):
        raise ValueError("Missing required auth parameters: clientId, clientSecret, or tenantId")
    
    # Get access token using client credentials from request
    access_token = api_service.get_access_token(client_id, client_secret, tenant_id)
    
    @dlt.resource(name="mailboxes", write_disposition="replace", primary_key="id")
    def get_mailboxes() -> Iterator[Dict[str, Any]]:
        """Extract mailbox information for all users in tenant"""
        logger = logging.getLogger(__name__)
        
        try:
            logger.info("Starting mailbox extraction")
            data = api_service.get_mailboxes(access_token, limit=1000)
            
            mailbox_count = 0
            for mailbox in data.get('value', []):
                # Add extraction metadata
                mailbox.update({
                    '_extracted_at': datetime.now(timezone.utc).isoformat(),
                    '_scan_id': filters.get('scan_id'),
                    '_tenant_id': filters.get('organization_id'),
                    '_extraction_type': 'mailbox'
                })
                yield mailbox
                mailbox_count += 1
            
            logger.info(f"Mailbox extraction completed. Total mailboxes: {mailbox_count}")
            
        except Exception as e:
            logger.error(f"Error extracting mailboxes: {e}")
            raise
    
    @dlt.resource(name="emails", write_disposition="replace", primary_key="id")
    def get_emails() -> Iterator[Dict[str, Any]]:
        """Extract emails from all mailboxes in the tenant"""
        logger = logging.getLogger(__name__)
        job_id = filters.get('scan_id', 'unknown')
        
        # Configure folders to scan (can be customized via filters)
        folders_to_scan = filters.get('folders', ['inbox'])  # Default to inbox only
        
        # Resume from checkpoint if available
        if resume_from:
            current_user_index = resume_from.get('checkpoint_data', {}).get('current_user_index', 0)
            current_folder_index = resume_from.get('checkpoint_data', {}).get('current_folder_index', 0)
            skip_token = resume_from.get('cursor')
            total_records = resume_from.get('records_processed', 0)
            logger.info(f"Resuming extraction from user {current_user_index}, folder {current_folder_index}, token: {skip_token}")
        else:
            current_user_index = 0
            current_folder_index = 0
            skip_token = None
            total_records = 0
            logger.info("Starting fresh email extraction")
        
        try:
            # First get all mailboxes
            logger.info("Getting mailbox list for email extraction")
            mailboxes_data = api_service.get_mailboxes(access_token)
            mailboxes = mailboxes_data.get('value', [])
            
            if not mailboxes:
                logger.warning("No mailboxes found in tenant")
                return
            
            logger.info(f"Found {len(mailboxes)} mailboxes to process")
            
            # Process emails from each mailbox
            for user_index, mailbox in enumerate(mailboxes[current_user_index:], current_user_index):
                user_id = mailbox['id']
                user_email = mailbox.get('mail', mailbox.get('userPrincipalName', 'unknown'))
                display_name = mailbox.get('displayName', 'Unknown User')
                
                logger.info(f"Processing emails for user {user_index + 1}/{len(mailboxes)}: {display_name} ({user_email})")
                
                # Process each folder for this user
                for folder_index, folder in enumerate(folders_to_scan[current_folder_index:], current_folder_index):
                    if user_index > current_user_index:
                        # Reset folder index for new user
                        folder_index = 0
                        current_folder_index = 0
                    
                    logger.info(f"  Processing folder: {folder}")
                    
                    # Reset skip_token for new folder unless resuming
                    if folder_index > current_folder_index or user_index > current_user_index:
                        skip_token = None
                    
                    page_count = 0
                    user_folder_email_count = 0
                    
                    while True:
                        try:
                            # Get emails for this user and folder
                            emails_data = api_service.get_emails(
                                access_token, 
                                user_id, 
                                folder=folder, 
                                limit=100,  # Microsoft Graph recommended batch size
                                skip_token=skip_token
                            )
                            
                            emails = emails_data.get('value', [])
                            if not emails:
                                logger.info(f"    No more emails in {folder} for user: {user_email}")
                                break
                            
                            # Process and yield emails
                            for email in emails:
                                # Add extraction metadata
                                email.update({
                                    '_extracted_at': datetime.now(timezone.utc).isoformat(),
                                    '_scan_id': filters.get('scan_id'),
                                    '_tenant_id': filters.get('organization_id'),
                                    '_user_email': user_email,
                                    '_user_id': user_id,
                                    '_user_display_name': display_name,
                                    '_folder': folder,
                                    '_extraction_type': 'email'
                                })
                                yield email
                                total_records += 1
                                user_folder_email_count += 1
                            
                            page_count += 1
                            logger.debug(f"    Processed page {page_count} for {user_email}/{folder}: {len(emails)} emails")
                            
                            # Save checkpoint every 5 pages (500 emails per user/folder combination)
                            if checkpoint_callback and page_count % 5 == 0:
                                next_skip_token = None
                                if '@odata.nextLink' in emails_data:
                                    next_link = emails_data['@odata.nextLink']
                                    parsed = urlparse(next_link)
                                    skip_token_params = parse_qs(parsed.query).get('$skiptoken')
                                    next_skip_token = skip_token_params[0] if skip_token_params else None
                                
                                checkpoint_data = {
                                    'phase': 'emails',
                                    'records_processed': total_records,
                                    'cursor': next_skip_token,
                                    'page_number': page_count,
                                    'batch_size': 100,
                                    'checkpoint_data': {
                                        'current_user_index': user_index,
                                        'current_folder_index': folder_index,
                                        'current_user_email': user_email,
                                        'current_user_id': user_id,
                                        'current_folder': folder,
                                        'user_folder_emails_processed': user_folder_email_count,
                                        'total_users': len(mailboxes),
                                        'total_folders': len(folders_to_scan)
                                    }
                                }
                                checkpoint_callback(job_id, checkpoint_data)
                                logger.info(f"Checkpoint saved: {total_records} total emails processed")
                            
                            # Check for next page
                            if '@odata.nextLink' not in emails_data:
                                break
                                
                            # Extract skip token from nextLink
                            next_link = emails_data['@odata.nextLink']
                            parsed = urlparse(next_link)
                            skip_token_params = parse_qs(parsed.query).get('$skiptoken')
                            skip_token = skip_token_params[0] if skip_token_params else None
                            
                            if not skip_token:
                                break
                                
                        except Exception as email_error:
                            logger.error(f"Error processing emails for user {user_email}, folder {folder}, page {page_count}: {email_error}")
                            
                            # Save error checkpoint but continue with next folder
                            if checkpoint_callback:
                                error_checkpoint = {
                                    'phase': 'emails_error',
                                    'records_processed': total_records,
                                    'cursor': skip_token,
                                    'checkpoint_data': {
                                        'error': str(email_error),
                                        'error_user_index': user_index,
                                        'error_folder_index': folder_index,
                                        'error_user_email': user_email,
                                        'error_folder': folder,
                                        'recovery_cursor': skip_token
                                    }
                                }
                                checkpoint_callback(job_id, error_checkpoint)
                            
                            # Continue with next folder instead of failing entire job
                            break
                    
                    logger.info(f"  Completed folder '{folder}' for {user_email}: {user_folder_email_count} emails")
                
                # Reset folder index for next user
                current_folder_index = 0
                logger.info(f"Completed user {display_name} ({user_email})")
            
            # Final checkpoint
            if checkpoint_callback:
                final_checkpoint = {
                    'phase': 'emails_completed',
                    'records_processed': total_records,
                    'cursor': None,
                    'checkpoint_data': {
                        'completion_status': 'success',
                        'total_users_processed': len(mailboxes),
                        'total_folders_processed': len(folders_to_scan),
                        'final_total': total_records
                    }
                }
                checkpoint_callback(job_id, final_checkpoint)
                
            logger.info(f"Email extraction completed. Total emails: {total_records} from {len(mailboxes)} users across {len(folders_to_scan)} folders")
            
        except Exception as e:
            logger.error(f"Error extracting emails: {e}")
            raise
    
    return [get_mailboxes, get_emails]
```

---

### Step 5: Update Extraction Service

#### 5.1 Edit `services/extraction_service.py`

**Find line ~15:**
```python
from .api_service import APIService
```

**Replace with:**
```python
from .outlook_api_service import OutlookAPIService
```

**Find around line 30:**
```python
self.api_service = APIService()
```

**Replace with:**
```python
self.api_service = OutlookAPIService()
```

**Find around line 35:**
```python
def __init__(self, config: Dict[str, Any], source_type: str = "hubspot_users"):
```

**Replace with:**
```python
def __init__(self, config: Dict[str, Any], source_type: str = "outlook_emails"):
```

---

### Step 6: Update API Schemas

#### 6.1 Edit `api/schemas.py`

Find the `AuthSchema` class around line 15:

**Replace:**
```python
class AuthSchema(Schema):
    """Authentication schema"""
    accessToken = fields.Str(
        required=True,
        validate=validate.Length(min=10),
        error_messages={'required': 'Access token is required'}
    )
```

**With:**
```python
class AuthSchema(Schema):
    """Authentication schema"""
    clientId = fields.Str(
        required=True,
        validate=validate.Length(min=10),
        error_messages={'required': 'Client ID is required'}
    )
    clientSecret = fields.Str(
        required=True,
        validate=validate.Length(min=10),
        error_messages={'required': 'Client Secret is required'}
    )
    tenantId = fields.Str(
        required=True,
        validate=validate.Length(min=10),
        error_messages={'required': 'Tenant ID is required'}
    )
```

---

## Testing & Validation

### Step 1: Restart Services
```bash
# Stop services
docker-compose down

# Rebuild and start
docker-compose up -d --build

# Check logs for any errors
docker-compose logs -f outlook_service
```

### Step 2: Test Health Check
```bash
curl http://localhost:5000/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-01-XX...",
  "service": "outlook_email_extraction",
  "pipeline": {
    "pipeline_name": "outlook_emails_extraction",
    "destination_type": "postgres",
    "is_active": true,
    "source_type": "outlook_emails"
  }
}
```

### Step 3: Test API Documentation
Open browser: `http://localhost:5000/docs/`

You should see Swagger UI with updated schemas showing `clientId`, `clientSecret`, and `tenantId` fields.

### Step 4: Test Credentials Validation
```bash
curl -X POST http://localhost:5000/api/v1/scan/start \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "scanId": "test-outlook-validation-001",
      "organizationId": "test-tenant-123",
      "type": ["email"],
      "auth": {
        "clientId": "your-azure-client-id",
        "clientSecret": "your-azure-client-secret",
        "tenantId": "your-azure-tenant-id"
      },
      "filters": {
        "folders": ["inbox"]
      }
    }
  }'
```

**Expected Success Response:**
```json
{
  "success": true,
  "scanId": "test-outlook-validation-001",
  "status": "running",
  "startTime": "2025-01-XX...",
  "message": "outlook_emails extraction scan initiated successfully"
}
```

**Expected Failure Response (invalid credentials):**
```json
{
  "success": false,
  "scanId": "test-outlook-validation-001",
  "error": "Invalid outlook_emails access credentials",
  "message": "Credential validation failed"
}
```

### Step 5: Monitor Scan Progress
```bash
# Check scan status
curl http://localhost:5000/api/v1/scan/test-outlook-validation-001/status

# Check detailed logs
docker-compose logs -f outlook_service
```

### Step 6: Verify Data Extraction
After the scan completes (status: "completed"):

```bash
# Get available tables
curl http://localhost:5000/api/v1/results/test-outlook-validation-001/tables

# Get mailboxes data
curl "http://localhost:5000/api/v1/results/test-outlook-validation-001/data?tableName=mailboxes&limit=10"

# Get emails data
curl "http://localhost:5000/api/v1/results/test-outlook-validation-001/data?tableName=emails&limit=10"
```

---

## Production Deployment

### Step 1: Production Environment Setup

#### 1.1 Create Production Environment File
```bash
cp .env .env.production
```

#### 1.2 Edit `.env.production`
```env
# Production Configuration
FLASK_ENV=production
FLASK_DEBUG=false
SECRET_KEY=your-32-character-production-secret-key-here
DB_PASSWORD=secure-production-password

# Service Configuration
DLT_PIPELINE_NAME=outlook_email_extraction
DB_NAME=outlook_data_prod
DB_SCHEMA=outlook

# Microsoft Graph API
OUTLOOK_API_BASE_URL=https://graph.microsoft.com
OUTLOOK_API_TIMEOUT=30

# Production Optimizations
MAX_CONCURRENT_SCANS=10
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
LOG_LEVEL=INFO

# Security
CORS_ORIGINS=https://your-frontend-domain.com
```

### Step 2: Production Deployment

#### 2.1 Deploy with Docker Compose
```bash
# Use production compose file
docker-compose -f docker-compose.prod.yml up -d --build

# Check production health
curl https://your-production-domain.com/health
```

#### 2.2 Production Docker Compose Override
Create `docker-compose.prod.yml`:

```yaml
version: '3.8'

services:
  outlook_service:
    build:
      context: .
      dockerfile: Dockerfile.prod
    environment:
      - FLASK_ENV=production
      - SECRET_KEY=${SECRET_KEY}
      - DB_PASSWORD=${DB_PASSWORD}
      - OUTLOOK_API_BASE_URL=https://graph.microsoft.com
      - OUTLOOK_API_TIMEOUT=30
      - MAX_CONCURRENT_SCANS=10
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    restart: unless-stopped

  postgres:
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
    restart: unless-stopped
```

### Step 3: Production Monitoring

#### 3.1 Health Check Monitoring
Set up monitoring for these endpoints:
- `GET /health` - Overall service health
- `GET /api/v1/pipeline/info` - Pipeline status
- `GET /api/v1/scan/statistics` - Job statistics

#### 3.2 Log Monitoring
Monitor these log patterns:
```bash
# Successful extractions
grep "Email extraction completed" /var/log/outlook-service/app.log

# Authentication failures
grep "Credential validation failed" /var/log/outlook-service/app.log

# Rate limiting
grep "Rate limited" /var/log/outlook-service/app.log

# Errors
grep "ERROR" /var/log/outlook-service/app.log
```

---

## Troubleshooting

### Common Issues and Solutions

#### 1. Authentication Errors

**Issue**: `"Invalid outlook_emails access credentials"`

**Causes & Solutions:**
```bash
# Check if Azure app has correct permissions
# Required: User.Read.All, Mail.Read.All (Application permissions)

# Verify admin consent was granted
# In Azure Portal: App Registration > API permissions > Grant admin consent

# Test credentials manually
curl -X POST https://login.microsoftonline.com/YOUR_TENANT_ID/oauth2/v2.0/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "client_id=YOUR_CLIENT_ID&client_secret=YOUR_CLIENT_SECRET&scope=https://graph.microsoft.com/.default&grant_type=client_credentials"
```

#### 2. Rate Limiting Issues

**Issue**: Frequent rate limiting from Microsoft Graph

**Solutions:**
```python
# Reduce batch size in data_source.py
emails_data = api_service.get_emails(
    access_token, 
    user_id, 
    folder=folder, 
    limit=50,  # Reduced from 100
    skip_token=skip_token
)

# Increase checkpoint frequency
if checkpoint_callback and page_count % 3 == 0:  # Every 3 pages instead of 5

# Add delay between requests
import time
time.sleep(0.5)  # 500ms delay between API calls
```

#### 3. Memory Issues with Large Tenants

**Issue**: Service runs out of memory with large tenants

**Solutions:**
```python
# Process users in batches
def get_mailboxes_batch(api_service, access_token, batch_size=100):
    skip_token = None
    while True:
        data = api_service.get_mailboxes(access_token, limit=batch_size, skip_token=skip_token)
        yield data.get('value', [])
        
        if '@odata.nextLink' not in data:
            break
        # Extract skip token logic...

# Increase checkpoint frequency
checkpoint_interval = 2  # Save every 2 pages for large tenants
```

#### 4. Database Connection Issues

**Issue**: `"Database connection failed"`

**Solutions:**
```bash
# Check database container
docker-compose ps
docker-compose logs postgres

# Test direct connection
docker exec -it outlook_postgres pg_isready -U postgres -d outlook_data

# Check connection parameters
echo $DB_HOST $DB_PORT $DB_NAME $DB_USER

# Verify database schema exists
docker exec -it outlook_postgres psql -U postgres -d outlook_data -c "\dn"
```

#### 5. Job Stuck in "Running" State

**Issue**: Job shows as "running" but no progress

**Solutions:**
```bash
# Check for crashed jobs
curl -X POST http://localhost:5000/api/v1/maintenance/detect-crashed?timeoutMinutes=10

# Check service logs for errors
docker-compose logs -f outlook_service | grep ERROR

# Check latest checkpoint
curl http://localhost:5000/api/v1/scan/your-scan-id/status | jq '.checkpointInfo'

# Cancel and restart if needed
curl -X POST http://localhost:5000/api/v1/scan/your-scan-id/cancel
```

#### 6. Missing Permissions Error

**Issue**: `"Insufficient privileges to complete the operation"`

**Solutions:**
1. **Verify Application Permissions** in Azure Portal:
   - Go to Azure AD > App registrations > Your App
   - API permissions should show:
     - Microsoft Graph: `Mail.Read.All` (Application)
     - Microsoft Graph: `User.Read.All` (Application)
   
2. **Grant Admin Consent**:
   - Click "Grant admin consent for [Tenant Name]"
   - Wait 5-10 minutes for propagation

3. **Check Tenant Settings**:
   - Some tenants disable application-only access
   - Contact your Azure AD administrator

#### 7. Empty Results

**Issue**: Scan completes but no emails extracted

**Possible Causes & Solutions:**
```bash
# Check if users have mailboxes
curl "http://localhost:5000/api/v1/results/your-scan-id/data?tableName=mailboxes&limit=10"

# Verify folder names (some tenants use localized names)
# Try different folder names in your request:
{
  "filters": {
    "folders": ["inbox", "Inbox", "INBOX"]
  }
}

# Check user account status
# The API filters for accountEnabled=true, check if users are active

# Test with a specific user
# Use Graph Explorer: https://developer.microsoft.com/graph/graph-explorer
# Test: GET /users/{user-id}/mailFolders/inbox/messages
```

### Debugging Steps

#### 1. Enable Debug Logging
```bash
# Set environment variable
export LOG_LEVEL=DEBUG

# Restart service
docker-compose restart outlook_service

# Monitor detailed logs
docker-compose logs -f outlook_service
```

#### 2. Test Individual Components

**Test Authentication:**
```bash
curl -X POST http://localhost:5000/api/v1/scan/start \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "scanId": "auth-test-001",
      "organizationId": "test",
      "type": ["email"],
      "auth": {
        "clientId": "your-client-id",
        "clientSecret": "your-client-secret", 
        "tenantId": "your-tenant-id"
      }
    }
  }'
```

**Test Database Connection:**
```bash
curl http://localhost:5000/health | jq '.pipeline.database_health'
```

**Test API Documentation:**
```bash
curl http://localhost:5000/docs/ | grep "clientId\|clientSecret\|tenantId"
```

#### 3. Database Inspection

**Connect to Database:**
```bash
docker exec -it outlook_postgres psql -U postgres -d outlook_data
```

**Check Job Status:**
```sql
SELECT id, status, "recordsExtracted", "errorMessage", "startTime" 
FROM jobs 
ORDER BY "startTime" DESC 
LIMIT 10;
```

**Check Checkpoints:**
```sql
SELECT job_id, phase, "recordsProcessed", "createdAt", checkpoint_data
FROM job_checkpoints 
ORDER BY "createdAt" DESC 
LIMIT 10;
```

**Check Extracted Data:**
```sql
-- List all schemas
\dn

-- Connect to tenant schema
\c outlook_data
SET search_path TO outlook_emails_test_tenant_123;

-- Check tables
\dt

-- Check data
SELECT COUNT(*) FROM mailboxes;
SELECT COUNT(*) FROM emails;
SELECT * FROM emails LIMIT 3;
```

---

## Performance Optimization

### For Large Tenants (1000+ Users)

#### 1. Batch Configuration
```python
# In data_source.py, reduce batch sizes
emails_data = api_service.get_emails(
    access_token, 
    user_id, 
    limit=50,  # Reduced batch size
    skip_token=skip_token
)

# Increase checkpoint frequency
if checkpoint_callback and page_count % 2 == 0:  # Every 2 pages
```

#### 2. Resource Allocation
```yaml
# In docker-compose.prod.yml
services:
  outlook_service:
    deploy:
      resources:
        limits:
          memory: 8G      # Increased memory
          cpus: '4.0'     # More CPU cores
```

#### 3. Database Optimization
```env
# Increase connection pool
DB_POOL_SIZE=30
DB_MAX_OVERFLOW=60

# Longer timeouts
DB_POOL_TIMEOUT=60
```

### For High-Frequency Scanning

#### 1. Incremental Extraction
```python
# Add date filters to reduce data volume
def get_emails_incremental(self, access_token: str, user_id: str, 
                          since_date: str = None):
    params = {
        '$top': 100,
        '$select': 'id,subject,receivedDateTime,...',
        '$orderby': 'receivedDateTime desc'
    }
    
    if since_date:
        params['$filter'] = f'receivedDateTime gt {since_date}'
```

#### 2. Parallel Processing
```python
# Process multiple users concurrently (advanced)
import asyncio
import aiohttp

async def process_users_parallel(users, max_concurrent=5):
    semaphore = asyncio.Semaphore(max_concurrent)
    tasks = [process_user(user, semaphore) for user in users]
    await asyncio.gather(*tasks)
```

---

## API Usage Examples

### Basic Email Extraction
```bash
curl -X POST http://localhost:5000/api/v1/scan/start \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "scanId": "outlook-emails-tenant1-2025-01",
      "organizationId": "tenant1", 
      "type": ["email"],
      "auth": {
        "clientId": "12345678-1234-1234-1234-123456789012",
        "clientSecret": "your-client-secret-here",
        "tenantId": "87654321-4321-4321-4321-210987654321"
      },
      "filters": {
        "folders": ["inbox"]
      }
    }
  }'
```

### Multi-Folder Extraction
```bash
curl -X POST http://localhost:5000/api/v1/scan/start \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "scanId": "outlook-full-scan-tenant1-2025-01",
      "organizationId": "tenant1",
      "type": ["email"], 
      "auth": {
        "clientId": "your-client-id",
        "clientSecret": "your-client-secret",
        "tenantId": "your-tenant-id"
      },
      "filters": {
        "folders": ["inbox", "sent", "drafts"]
      }
    }
  }'
```

### Check Extraction Results
```bash
# Get scan status
curl http://localhost:5000/api/v1/scan/outlook-emails-tenant1-2025-01/status

# Get available tables
curl http://localhost:5000/api/v1/results/outlook-emails-tenant1-2025-01/tables

# Get mailboxes
curl "http://localhost:5000/api/v1/results/outlook-emails-tenant1-2025-01/data?tableName=mailboxes&limit=50"

# Get emails with pagination
curl "http://localhost:5000/api/v1/results/outlook-emails-tenant1-2025-01/data?tableName=emails&limit=100&offset=0"

# Get recent emails
curl "http://localhost:5000/api/v1/results/outlook-emails-tenant1-2025-01/data?tableName=emails&limit=20" | jq '.data.records[].subject'
```

---

## Next Steps

### 1. âœ… Complete Implementation
Follow all steps in this guide exactly - don't skip any steps.

### 2. ðŸ§ª Test with Small Tenant First  
Start with a test tenant that has only a few users to validate the implementation.

### 3. ðŸ“Š Monitor Performance
Watch memory usage, API rate limits, and extraction speed during initial runs.

### 4. ðŸ”§ Customize for Your Needs
- Add additional folders (sent, drafts, deleted)
- Implement incremental extraction for regular syncing
- Add custom filters based on date ranges or email properties

### 5. ðŸš€ Production Deployment
Use the production configuration and monitoring setup provided.

### 6. ðŸ“ˆ Scale and Optimize
Based on your tenant sizes and extraction frequency, apply the performance optimizations.

---

## Summary

This implementation provides a complete Microsoft Outlook email extraction service with:

âœ… **Full Microsoft Graph integration** with proper OAuth2 authentication  
âœ… **Robust error handling** and automatic retry for rate limits  
âœ… **Checkpointing system** for reliable resumption after failures  
âœ… **Multi-tenant support** with isolated data storage  
âœ… **Production-ready deployment** with Docker and monitoring  
âœ… **Comprehensive testing** and troubleshooting guidance  

The service can handle tenants of any size and provides reliable email extraction with full auditability and monitoring capabilities.